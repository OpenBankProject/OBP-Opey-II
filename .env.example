# Directory where the endpoint and glossary vector databases are
CHROMADB_DIRECTORY="./src/data/chroma_langchain_db"

# Open Bank Project Config
OBP_BASE_URL="https://apisandbox.openbankproject.com"
OBP_API_VERSION="v5.1.0"

# Open Bank Project API Credentials
OBP_USERNAME="your-obp-username"
OBP_PASSWORD="your-obp-password"
OBP_CONSUMER_KEY="your-obp-consumer-key"

## Server Config
# Mode to run server in for hot-reloading
MODE="dev"
PORT=5000
JWT_SIGNING_SECRET="very-very-secret"
# Set the CORS allowed origins to whatever frontends will be communicating with Opey, here is the default localhost and port for API Explorer II
CORS_ALLOWED_ORIGINS='["http://localhost:5173"]'

#Langhain Tracing
LANGCHAIN_TRACING_V2="false"
LANGCHAIN_API_KEY="lsv2_pt_..."
LANGCHAIN_PROJECT="langchain-opey"

# SelfRAG Retriever Config
ENDPOINT_RETRIEVER_BATCH_SIZE=8
ENDPOINT_RETRIEVER_MAX_RETRIES=2
# If there are less than this number of endpoints found for a given retrieval, retry with rewritten question
ENDPOINT_RETRIEVER_RETRY_THRESHOLD=1

# Number of conversation tokens at which we trim the messages and summarize the conversation
CONVERSATION_TOKEN_LIMIT=50000

# If true, this disables the tools allowing Opey to call OBP API, I.e returns it to an "Opey 1 - like" state
# A purely informational agent based on the OBP Resource docs and glossary
DISABLE_OBP_CALLING=true

# Model Config
# Currently supported are "openai", "anthropic", and "ollama"
MODEL_PROVIDER="openai"

OPENAI_SMALL_MODEL="gpt-4o-mini"
OPENAI_MEDIUM_MODEL="gpt-4o"

ANTHROPIC_SMALL_MODEL="claude-3-haiku-20240307"
ANTHROPIC_MEDIUM_MODEL="claude-3-sonnet-20240229"
# It is very possible to set both sizes to the same model, if you do not care about cost
# Also note that the models chosen here will need to support tool calling: https://ollama.com/search?&c=tools
OLLAMA_SMALL_MODEL="llama3.2"
OLLAMA_MEDIUM_MODEL="llama3.2"

# Model API
OPENAI_API_KEY="sk-proj-..."
ANTHROPIC_API_KEY="sk-ant-api03-..."

# Default LLM Config, NOTE that you will need the API key for whatever the model provider is
DEFAULT_LLM_MODEL="gpt-4o"
DEFAULT_LLM_TEMPERATURE=0.5